{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2oeA5rzrGmZ",
        "outputId": "114c4dd3-c7f8-4631-ca61-448407c12484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "--2023-12-13 06:03:04--  https://raw.githubusercontent.com/camenduru/Qwen-VL-Chat-colab/main/app.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9816 (9.6K) [text/plain]\n",
            "Saving to: ‘/content/app.py’\n",
            "\n",
            "\r/content/app.py       0%[                    ]       0  --.-KB/s               \r/content/app.py     100%[===================>]   9.59K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-12-13 06:03:04 (114 MB/s) - ‘/content/app.py’ saved [9816/9816]\n",
            "\n",
            "2023-12-13 06:03:26,720 - modelscope - INFO - PyTorch version 2.1.0+cu118 Found.\n",
            "2023-12-13 06:03:26,721 - modelscope - INFO - TensorFlow version 2.14.0 Found.\n",
            "2023-12-13 06:03:26,721 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
            "2023-12-13 06:03:26,762 - modelscope - INFO - Loading done! Current index file version is 1.10.0, with md5 61ceccc5c5e9643ad8565c57dc1ca56b and a total number of 946 components indexed\n",
            "2023-12-13 06:03:28.775945: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-13 06:03:28.776000: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-13 06:03:28.776039: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-13 06:03:30.811723: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Fetching 20 files: 100% 20/20 [00:00<00:00, 131896.35it/s]\n",
            "CUDA extension not installed.\n",
            "CUDA extension not installed.\n",
            "Loading checkpoint shards: 100% 5/5 [00:46<00:00,  9.40s/it]\n",
            "/content/app.py:203: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
            "  chatbot = gr.Chatbot(label='Qwen-VL-Chat', elem_classes=\"control-height\").style(height=500)\n",
            "Running on local URL:  http://127.0.0.1:8000\n",
            "Running on public URL: https://8376616a93808bcfa7.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "User: describe the image\n",
            "Qwen-VL-Chat: <ref> Two coworkers</ref><box>(316,60),(750,974)</box><box>(0,45),(446,996)</box> high fiving each other in office\n",
            "User: describe the image\n",
            "Qwen-VL-Chat: <ref> Business colleagues</ref><box>(846,162),(997,747)</box><box>(314,53),(748,963)</box><box>(0,46),(447,996)</box> giving each other a high five in office\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2361, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/app.py\", line 245, in <module>\n",
            "    main()\n",
            "  File \"/content/app.py\", line 241, in main\n",
            "    _launch_demo(args, model, tokenizer)\n",
            "  File \"/content/app.py\", line 228, in _launch_demo\n",
            "    demo.queue().launch(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2266, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2365, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/networking.py\", line 75, in close\n",
            "    self.thread.join()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:8000 <> https://8376616a93808bcfa7.gradio.live\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!wget https://raw.githubusercontent.com/camenduru/Qwen-VL-Chat-colab/main/app.py -O /content/app.py\n",
        "\n",
        "!pip install -q tiktoken transformers_stream_generator gradio==3.50.2 optimum auto-gptq huggingface_hub\n",
        "!pip install -q modelscope -f https://pypi.org/project/modelscope\n",
        "\n",
        "!python app.py --share"
      ]
    }
  ]
}